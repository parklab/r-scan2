setClassUnion('null.or.df', c('NULL', 'data.frame'))
setClassUnion('null.or.dt.or.raw', c('NULL', 'data.table', 'raw'))
setClassUnion('null.or.Seqinfo', c('NULL', 'Seqinfo'))
setClassUnion('null.or.GRanges', c('NULL', 'GRanges'))
setClassUnion('null.or.list', c('NULL', 'list'))
setClassUnion('null.or.logical', c('NULL', 'logical'))
setClassUnion('null.or.character', c('NULL', 'character'))

# if adding new slots to the class, be sure to update concat(...)
# to combine the slots properly.
setClass("SCAN2", slots=c(
    package.version='null.or.character',
    pipeline.version='null.or.character',
    config='null.or.list',
    region='null.or.GRanges',
    analysis.regions='null.or.GRanges',
    genome.string='character',
    genome.seqinfo='null.or.Seqinfo',
    single.cell='character',
    bulk='character',
    sex='character',
    amplification='character',
    gatk="null.or.dt.or.raw",
    integrated.table.path='null.or.character',
    ab.fits='null.or.df',
    static.filter.params='null.or.list',
    ab.estimates='null.or.df',
    mut.models='null.or.df',
    cigar.data='null.or.df',
    excess.cigar.scores='null.or.list',
    fdr.prior.data='null.or.list',
    fdr='null.or.list',
    call.mutations='null.or.list',
    depth.profile='null.or.list',
    binned.counts='null.or.list',
    mutburden='null.or.list',
    mutsig.rescue='null.or.list',
    spatial.sensitivity='null.or.list'))


# Just a wrapper on read_yaml for now.  Maybe some other handy parsing (or
# checking) later.
read.config <- function(file.name) {
    yaml::read_yaml(file.name)
}

# "flip" the GP mu around 0 to best match the AF of each candidate mutation. 
match.ab <- function(af, gp.mu) {
    ifelse(af < 1/2, -abs(gp.mu), abs(gp.mu))
}

# currently will always fire when SCAN2 is built from chunked objects.
# firing isn't a problem, just causes a warning message.
# need to define what the "full genome" is (in terms of GRanges objects)
# for the recognized genomes.
check.chunked <- function(object, message) {
    if (is.null(object@analysis.regions))
        warning('slot @analysis.regions not found.  This SCAN2 object was probably generated by an older (incompatible) SCAN2 package.')
    if (is.null(object@region))
        warning('slot @region is NULL.  Probably safe to ignore this message.')

    not.covered <- GenomicRanges::countOverlaps(object@analysis.regions, object@region, type='within') == 0
    if (any(not.covered))
        warning(paste('this object may be a chunked (for parallelization) object.  found', sum(not.covered), 'analysis regions not covered:', message))
}

get.rscan2.version <- function() {
    v <- utils::packageDescription('scan2')

    # If r-scan2 was installed by devtools:install_github(), the GithubSHA1
    # will be recorded in the package information. If so, return the commit
    # short tag.
    if ('GithubSHA1' %in% names(v)) {
        commit.id <- v$GithubSHA1
        attr(commit.id, 'source') <- 'github_commit'
        return(commit.id)
    } else {
        version.string <- paste(v$Version, v$Date, sep='-')
        attr(version.string, 'source') <- 'package_metadata'
        return(version.string)
    }
}


# The user must supply a SCAN2 configuration as either:
#       1. a path to the .yaml file or
#       2. the parsed list stored in another SCAN2 object's @config slot
# Only one of the above two methods can be used.
# "Pipeline" versioning refers to the external snakemake pipeline that
# prepares input for the R SCAN2 library to analyze.
make.scan <- function(config, config.path, single.cell='NOT_SPECIFIED_BY_USER', region=NULL,
    pipeline.version=NA, pipeline.buildnum=NA, pipeline.githash=NA)
{
    if (!missing(config) & !missing(config.path))
        stop('only one of `config` or `config.path` can be specified, but was called with both')

    # otherwise, config is already what we want
    if (!missing(config.path))
        config <- read.config(config.path)

    # Single cells may be missing from the amplification list.
    amplification <- toupper(config$amplification[single.cell])
    # toupper() converts NULL -> "NULL"
    if (amplification == "NULL")
        amplification <- 'UNKNOWN'

    object <- new("SCAN2",
        package.version=get.rscan2.version(),
        pipeline.version=c(
            # Need as.character in case no version info supplied: (NA,NA,NA) is type=logical
            version=as.character(pipeline.version),
            buildnum=as.character(pipeline.buildnum),
            githash=as.character(pipeline.githash)),
        config=config,
        sex=tolower(config$sex),
        amplification=amplification,
        single.cell=single.cell,
        bulk=config$bulk_sample,
        genome.string=config$genome,
        # calling genome.string.to.* will ensure genome.string is valid
        genome.seqinfo=genome.string.to.seqinfo.object(config$genome),
        region=region,
        static.filter.params=parse.static.filter.params(config),
        analysis.regions=parse.analysis.regions.to.granges(config),
        depth.profile=NULL,
        binned.counts=NULL,
        gatk=NULL,
        ab.fits=NULL,
        ab.estimates=NULL,
        mut.models=NULL,
        cigar.data=NULL,
        excess.cigar.scores=NULL,
        fdr.prior.data=NULL,
        fdr=NULL,
        call.mutations=NULL,
        mutburden=NULL,
        mutsig.rescue=NULL,
        spatial.sensitivity=NULL)
    object
}


# Extract all static filter params from the configuration file
parse.static.filter.params <- function(config) {
    setNames(lapply(c('snv', 'indel'), function(mt) {
        list(
            min.sc.alt=config[[paste0(mt, '_min_sc_alt')]],
            min.sc.dp=config[[paste0(mt, '_min_sc_dp')]],
            max.bulk.alt=config[[paste0(mt, '_max_bulk_alt')]],
            max.bulk.af=config[[paste0(mt, '_max_bulk_af')]],
            max.bulk.binom.prob=config[[paste0(mt, '_max_bulk_binom_prob')]],
            min.bulk.dp=config[[paste0(mt, '_min_bulk_dp')]],
            exclude.dbsnp=config[[paste0(mt, '_exclude_dbsnp')]],
            cg.id.q=config[[paste0(mt, '_cigar_id_score_quantile_cutoff')]],
            cg.hs.q=config[[paste0(mt, '_cigar_hs_score_quantile_cutoff')]],
            # Some cross-sample panel parameters that are currently not configurable
            # via the usual process. For now, use update.static.filter.params() to change.
            panel.use=ifelse(mt == 'snv', FALSE, TRUE),    # apply the panel or not? 
            # IMPORTANT!! allow.missing was historically FALSE.  It is only recommended
            # for very large panels.
            panel.allow.missing=TRUE,                   # if a site is missing, does it PASS?
            panel.max.unique.donors=1,
            panel.max.out=2
        )
    }), c('snv', 'indel'))
}


# Format of GATK intervals: chrom:start-end
# positions are 1-indexed, inclusive on both sides
parse.analysis.regions.to.granges <- function(config) {
    split1 <- strsplit(config$analysis_regions, ':')
    split2 <- strsplit(sapply(split1, `[`, 2), '-')

    # reduce(): the full range data is still stored in config if needed later
    GenomicRanges::reduce(GenomicRanges::GRanges(
        seqnames=sapply(split1, `[`, 1),
        ranges=IRanges::IRanges(start=as.integer(sapply(split2, `[`, 1)),
                                end=as.integer(sapply(split2, `[`, 2))),
        seqinfo=genome.string.to.seqinfo.object(config$genome)))
}


setValidity("SCAN2", function(object) {
    if (length(object@genome.string) != 1)
        return("must provide exactly one genome name")

    if (!is.null(object@genome.seqinfo)) {
        if (class(object@genome.seqinfo) != 'Seqinfo')
            stop('@genome.seqinfo must be of class Seqinfo')
    }

    if (length(object@single.cell) != 1)
        return("must provide exactly one single cell sample name")

    if (length(object@bulk) != 1)
        return("must provide exactly one bulk sample name")

    if (!is.null(object@region)) {
        if (!is(object@region, 'GRanges')) 
            stop('"region" must be a GRanges object')
        # This isn't possible if we're to combine chunks for the final
        # pipeline steps. Need a check that allows either single region
        # GRanges or the expected full autosome set.
        #
        # Disabling for now, but this isn't safe in general.
        #if (length(object@region) != 1) {
           #stop('"region" must contain exactly 1 range')
        #}
    }

    if (!is.null(object@gatk)) {
        if (ncol(object@gatk) != 13) {
            return("@gatk must have 13 columns")
        }
        if (!all(colnames(object@gatk)[1:7] ==
                c('chr', 'pos', 'refnt', 'altnt', 'dbsnp', 'mq', 'mqrs'))) {
            return("@gatk is improperly formatted")
        }
    }

    if (!is.null(object@ab.fits)) {
        # XXX: should also check that relevant chromosomes are present
        if (!all(colnames(object@ab.fits == c('a', 'b', 'c', 'd', 'logp'))))
            return("@ab.fits is improperly formatted")
    }

    if (!is.null(object@ab.estimates)) {
        if (!all(c('ab', 'gp.mu', 'gp.sd') %in% colnames(object@gatk)))
            return("ab.estimates were not properly imported")
    }

    if (!is.null(object@mut.models)) {
        if (!all(c('abc.pv', 'lysis.pv', 'lysis.beta', 'mda.pv', 'mda.beta')
            %in% colnames(object@gatk)))
            return("mut.models were not properly calculated")
    }

    # XXX: add validator for rest of slots
    return(TRUE)
})


# various steps in the pipeline require certain slots to be filled.
# this function will ensure all slots in 'slots: (character)' are not
# NULL and will generate an error message otherwise.
check.slots <- function(object, slots, abort=TRUE) {
    error.occurred = FALSE
    for (s in slots) {
        if (is.null(slot(object, s))) {
            error.occurred = TRUE
            if (s == 'gatk')
                cat("must import GATK read counts first (see: read.integrated.table())\n")
            if (s == 'ab.fits')
                cat('must import AB model parameters (see: add.ab.fits())\n')
            if (s == 'ab.estimates')
                cat("must import allele balance model estimates first (see: add.ab.estimates())\n")
            if (s == 'cigar.data')
                cat("must import CIGAR data first (see: add.cigar.data())\n")
            if (s == 'excess.cigar.scores')
                cat("must compute excess CIGAR scores first (see: compute.excess.cigar.scores())\n")
            if (s == 'static.filter.params')
                cat("must apply static site filters first (see: add.static.filters())\n")
            if (s == 'fdr.prior.data')
                cat("must compute or import FDR priors first (see: compute.fdr.prior.data())\n")
        }
    }
    if (error.occurred & abort)
        stop("One or more required slots are missing. See above for details.")
    return(error.occurred)
}


# to use: e.g., x <- do.call(concat, xs)
# where xs is a list of SCAN2 chunks. the chunks must not overlap. one day this
# might be enforced programatically
setGeneric("concat", function(...) standardGeneric("concat"))
setMethod("concat", signature="SCAN2", function(...) {
    args <- list(...)

    if (length(args) == 0)
        stop('tried to concat() 0 objects')

    # This may seem like a good idea, but it is inconsistent behavior. For args>1
    # the code below creates a concatenated *copy* of the input data, while this
    # would not create a copy.  Copies are unfortunately necessary in counter-
    # intuitive ways. E.g., when returning `data.tables` (which are inside SCAN2
    # objects) from future-parallelized loops (e.g., future_lapply), the memory
    # hackery used by data.table to enable update-by-reference cannot be passed
    # back from forked R child processes. These returned data.tables will then
    # *silently* fail all update-by-ref operations (the `:=` operator), leading
    # to downstream errors when columns that should be present are missing.
    #if (length(args) == 1)
        #return(args[[1]])

    init <- args[[1]]

    ret <- make.scan(config=init@config, single.cell=init@single.cell,
        region=reduce(do.call(c, lapply(args, function(a) a@region))),
        pipeline.version=init@pipeline.version['version'],
        pipeline.buildnum=init@pipeline.version['buildnum'],
        pipeline.githash=init@pipeline.version['githash'])

    # rbindlist quickly concatenates data.tables
    ret@gatk <- rbindlist(lapply(args, function(a) a@gatk))

    ensure.same <- function(l, slot.name, var.name, var.name2) {
        if (missing(var.name) & !missing(var.name2))
            stop("var.name2 only valid if var.name also specified")

        # either all slots are NULL or they have the same value
        if (all(sapply(l, function(element) is.null(slot(element, slot.name))))) {
            return()
        }
        # if var.name isn't supplied, assume that `slot.name` is just a vector
        if (!missing(var.name)) {
            if (missing(var.name2)) {
                if (!all(sapply(l, function(element)
                    all(slot(l[[1]], slot.name)[[var.name]] == slot(element, slot.name)[[var.name]]))))
                    stop(paste('list of SCAN2 chunks cannot be concatenated; slot', slot.name,
                        'does not have consistent values for', var.name))
            } else {
                # very hackish way to allow nested testing
                if (!all(sapply(l, function(element)
                    all(slot(l[[1]], slot.name)[[var.name]][[var.name2]] == slot(element, slot.name)[[var.name]][[var.name2]]))))
                    stop(paste('list of SCAN2 chunks cannot be concatenated; slot', slot.name,
                        'does not have consistent values for', var.name))
            }
        } else {
            if (!all(sapply(l, function(element)
                all(slot(l[[1]], slot.name) == slot(element, slot.name)))))
                stop(paste('list of SCAN2 chunks cannot be concatenated; slot', slot.name,
                    'does not have consistent values'))
        }
    }

    # XXX: TODO: would be nice to ensure same genome object. just not sure how to
    # check equality at the moment.
    ensure.same(args, 'genome.string')
    ensure.same(args, 'single.cell')
    ensure.same(args, 'bulk')

    for (mt in c('snv', 'indel')) {
        ensure.same(args, 'static.filter.params', mt, 'min.sc.alt')
        ensure.same(args, 'static.filter.params', mt, 'min.sc.dp')
        ensure.same(args, 'static.filter.params', mt, 'min.bulk.dp')
        ensure.same(args, 'static.filter.params', mt, 'max.bulk.alt')
        ensure.same(args, 'static.filter.params', mt, 'max.bulk.af')
        ensure.same(args, 'static.filter.params', mt, 'exclude.dbsnp')
        ensure.same(args, 'static.filter.params', mt, 'cg.id.q')
        ensure.same(args, 'static.filter.params', mt, 'cg.hs.q')
        ensure.same(args, 'static.filter.params', mt, 'disable.hs.filter')
    }
    ret@static.filter.params <- init@static.filter.params

    ensure.same(args, 'cigar.data', 'sc.path')
    ensure.same(args, 'cigar.data', 'bulk.path')
    ret@cigar.data <- data.frame(sc.sites=sum(sapply(args, function(a) ifelse(is.null(a@cigar.data), 0, a@cigar.data$sc.sites))),
                               bulk.sites=sum(sapply(args, function(a) ifelse(is.null(a@cigar.data), 0, a@cigar.data$bulk.sites)))
    )

    for (mt in c('snv', 'indel')) {
        ensure.same(args, 'excess.cigar.scores', mt, 'legacy')
        ensure.same(args, 'excess.cigar.scores', mt, 'null.sites')
    }
    ret@excess.cigar.scores <- init@excess.cigar.scores
    ret@excess.cigar.scores$snv$sites <- sum(sapply(args, function(a) ifelse(is.null(a@excess.cigar.scores$snv), 0, a@excess.cigar.scores$snv$sites)))
    ret@excess.cigar.scores$indel$sites <- sum(sapply(args, function(a) ifelse(is.null(a@excess.cigar.scores$indel), 0, a@excess.cigar.scores$indel$sites)))

    # fdr.prior.data: 'fcs' should also be identical, but the list is a little
    # inconvenient to check. the below should detect misuse 99% of the time.
    for (mt in c('snv', 'indel')) {
        ensure.same(args, 'fdr.prior.data', mt, 'bins')
        ensure.same(args, 'fdr.prior.data', mt, 'max.dp')
        ensure.same(args, 'fdr.prior.data', mt, 'candidates.used')
        ensure.same(args, 'fdr.prior.data', mt, 'hsnps.used')
        ensure.same(args, 'fdr.prior.data', mt, 'nt.tab')
        ensure.same(args, 'fdr.prior.data', mt, 'na.tab')
        ensure.same(args, 'fdr.prior.data', mt, 'mode')
    }
    ret@fdr.prior.data <- init@fdr.prior.data

    ret@fdr <- NULL
    if (any(sapply(args, function(a) !is.null(a@fdr)))) {
        ret@fdr <- list()
        for (mt in c('snv', 'indel')) {
            ensure.same(args, 'fdr', mt, 'mode')
            ret@fdr[[mt]] <- data.frame(mode=init@fdr[[mt]]$mode,
                sites=sum(sapply(args, function(a) ifelse(is.null(a@fdr[[mt]]), 0, a@fdr[[mt]]$sites))))
        }
    }

    # policy: ab.fits has to be the same for all chunks being concat()ed
    # if that's true, just use the first one. don't try to as.matrix() a
    # NULL slot.
    if (!all(sapply(args, function(a) is.null(a@ab.fits)))) {
        if (any(sapply(args, function(a) any(as.matrix(init@ab.fits) != as.matrix(a@ab.fits)))))
            stop('@ab.fits must be identical for all concat() elements')
    }
    ret@ab.fits <- init@ab.fits

    ret@ab.estimates <- data.frame(sites=sum(sapply(args, function(a) ifelse(is.null(a@ab.estimates), 0, a@ab.estimates$sites))))

    ret@mut.models <- data.frame(sites=sum(sapply(args, function(a) ifelse(is.null(a@mut.models), 0, a@mut.models$sites))))

    ret
})



setGeneric("read.integrated.table", function(object, path, quiet=FALSE)
    standardGeneric("read.integrated.table"))
setMethod("read.integrated.table", "SCAN2", function(object, path, quiet=FALSE) {
    object@gatk <- read.and.annotate.integrated.table(path=path, sample.id=object@single.cell,
        region=object@region, quiet=quiet)
    object@integrated.table.path <- path

    id <- object@gatk[,paste(chr, pos, refnt, altnt)]
    dupid <- duplicated(id)
    if (sum(dupid)) {
        stop(paste('found', sum(dupid), 'duplicate (chr,pos,refnt,altnt) instances in integrated table. This is likely a bug, please report it.'))
    }

    object
})



# Add AB Gaussian process parameter fits for each chromosome.
# These take a long time to compute, so the snakemake pipeline will almost
# always be necessary here.
setGeneric("add.ab.fits", function(object, path)
    standardGeneric("add.ab.fits"))
setMethod("add.ab.fits", "SCAN2", function(object, path) {
    object@ab.fits <- get(load(path))
    object
})



# Using tile subsampling, AB model fitting can feasibly be performed on
# a single multicore node.
# AB fits must be computed using all training sites on a chromosome.
# AB fits are produced by sampling 20,000 random parameter 4-tuples. The
# highest logP values are considered the best fits. The random sampling
# is iteratively refined in 'n.steps' steps by restricting random parameter
# sampling to a smaller subset of the space.
#   - samples.per.chunk, n.chunks: compute 'samples.per.chunk' random
#         parameter samplings in 'n.chunks' independent (possibly parallel)
#         threads. samples.per.chunk*n.chunks should be kept at 20,000.
#   - refine.n.steps: number of iterations in which the (a,b,c,d) parameter space
#         is refined and sampled.
#   - refine.top.n: use the top 'top.n' parameter values (by logP) to create the
#         next refined parameter space
#   - n.tiles and hsnp.tilesize: the log-likelihood of (a,b,c,d|training hSNPs)
#         is approximated by breaking the hSNPs into non-overlapping tiles of
#         size hsnp.tilesize. This is necessary because the approximation requires
#         inverting the (#hSNPs x #hSNPs) covariance matrix. Furthermore, the
#         additional information about (a,b,c,d) provided by each tile of hSNPs
#         becomes less and less as more tiles are added. We have found that ~200
#         tiles (=20,000 hSNPs) is a good trade-off between compute time and
#         accuracy.
#   - alim, blim, clim, dlim - starting bounds for the (a,b,c,d) parameter
#         space.
setGeneric("compute.ab.fits", function(object, path, chroms, #=1:22,
    n.cores=future::availableCores(),
    logp.samples.per.step=20000, refine.n.steps=4, refine.top.n=50,
    n.tiles=250, hsnp.tilesize=100,
    alim=c(-7, 2), blim=c(2, 4), clim=c(-7, 2), dlim=c(2, 6))
    standardGeneric("compute.ab.fits"))
setMethod("compute.ab.fits", "SCAN2", function(object, path, chroms, #=1:22,
    n.cores=future::availableCores(),
    logp.samples.per.step=20000, refine.n.steps=4, refine.top.n=50,
    n.tiles=250, hsnp.tilesize=100,
    alim=c(-7, 2), blim=c(2, 4), clim=c(-7, 2), dlim=c(2, 6)) 
{
    cat("using", n.cores, "cores\n")
    n.chunks <- 100  # 4*n.cores  # using a multiple of n.cores gives a smoother progress bar
    # using n.cores and ceiling can lead to (very small) differences in number
    # of samples taken, and thus slightly different results. just use a very large
    # number of chunks (like 100)
    if (logp.samples.per.step %% n.chunks != 0)
        stop(sprintf('logp.samples.per.step must be a multiple of n.chunks (%d)', n.chunks))
    samples.per.chunk <- ceiling(logp.samples.per.step / n.chunks)

    # Check all chroms up front so the loop doesn't die after a significant amount of work
    not.in <- chroms[!(chroms %in% seqnames(object@genome.seqinfo))]
    if (length(not.in) > 0) {
        cat("the following chromosomes are not recognized:\n")
        print(not.in)
        cat("valid chromosomes names for genome", object@genome.string, 'are:\n')
        print(seqnames(object@genome.seqinfo))
        stop('invalid chromosomes, see above for details')
    }

    chrom.refine.records <- setNames(lapply(chroms, abmodel.fit.one.chrom,
        path=path, sc.sample=object@single.cell,
        genome.seqinfo=object@genome.seqinfo,
        hsnp.tilesize=hsnp.tilesize, n.tiles=n.tiles,
        refine.n.steps=refine.n.steps, n.chunks=n.chunks,
        n.logp.samples.per.chunk=samples.per.chunk), chroms)

    chrom.refine.records
})



# Actually calculate the AB estimates.
# Currently, computes AB at ALL SITES rather than just somatic candidates.
# This will be useful in the future for mosaics and perhaps for using more
# germline hSNPs to model what somatic mutations should look like.
#
# IMPORTANT: AB estimation benefits greatly from chunking. However,
# AB estimation uses a window of 100kb up and downstream from every site at
# which AB is being estimated. For sites at the edge of each chunk, data
# either upstream or downstream will not be available. To solve this,
# training data must be read in AGAIN with the 100kb flanking regions added.
#
# path - if not NULL, then assumes the file contains already-computed AB
#        estimates in a bgzipped, tabix-indexed file.
setGeneric("compute.ab.estimates", function(object, path=NULL, quiet=FALSE)
    standardGeneric("compute.ab.estimates"))
setMethod("compute.ab.estimates", "SCAN2", function(object, path=NULL, quiet=FALSE)
{
    check.slots(object, c('gatk', 'ab.fits'))

    if (is.null(path)) {
        training.hsnps <- get.training.sites.for.abmodel(object=object,
            region=object@region, integrated.table.path=object@integrated.table.path, quiet=quiet)

        ab <- compute.ab.given.sites.and.training.data(
            sites=object@gatk[,.(chr,pos,refnt,altnt)],
            training.hsnps=training.hsnps,
            ab.fits=object@ab.fits, quiet=quiet)
    } else {
        ab <- read.tabix.data(path=path, region=object@region, quiet=quiet)
        # Extra sanity test to ensure read-in data matches the table in this object
        if (nrow(ab) != nrow(object@gatk) || any(ab$chr != object@gatk$chr))
            stop('AB estimates file', path, 'does not exactly match internal table chromosomes')
        if (nrow(ab) != nrow(object@gatk) || any(ab$pos != object@gatk$pos))
            stop('AB estimates file', path, 'does not exactly match internal table positions')
        if (nrow(ab) != nrow(object@gatk) || any(ab$refnt != object@gatk$refnt))
            stop('AB estimates file', path, 'does not exactly match internal table refnt')
        if (nrow(ab) != nrow(object@gatk) || any(ab$altnt != object@gatk$altnt))
            stop('AB estimates file', path, 'does not exactly match internal table altnt')
    }

    # Add an extra 'ab' column that transforms gp.mu (-Inf, Inf) -> [0, 1]
    object@gatk[, c('ab', 'gp.mu', 'gp.sd') := 
        list(1/(1+exp(-ab$gp.mu)), ab$gp.mu, ab$gp.sd)]
    object@ab.estimates <- data.frame(sites=nrow(ab))
    object
})



# path - if not NULL, then assumes the file contains already-computed model
#        estimates in a bgzipped, tabix-indexed file.
setGeneric("compute.models", function(object, path=NULL, verbose=TRUE)
    standardGeneric("compute.models"))
setMethod("compute.models", "SCAN2", function(object, path=NULL, verbose=TRUE)
{
    check.slots(object, c('gatk', 'ab.estimates'))

    if (is.null(path)) {
        if (nrow(object@gatk) > 0) {
            matched.gp.mu <- match.ab(af=object@gatk$af, gp.mu=object@gatk$gp.mu)
            pvb <- compute.pvs.and.betas(object@gatk$scalt, object@gatk$dp,
                                        matched.gp.mu, object@gatk$gp.sd, verbose=verbose)
        } else {
            pvb <- data.table(abc.pv=numeric(0), lysis.pv=numeric(0), lysis.beta=numeric(0), mda.pv=numeric(0), mda.beta=numeric(0))
        }
    } else {
        pvb <- read.tabix.data(path=path, region=object@region, quiet=!verbose)
        # Extra sanity test to ensure read-in data matches the table in this object
        if (nrow(pvb) != nrow(object@gatk) || any(pvb$chr != object@gatk$chr))
            stop('model file', path, 'does not exactly match internal table chromosomes')
        if (nrow(pvb) != nrow(object@gatk) || any(pvb$pos != object@gatk$pos))
            stop('model file', path, 'does not exactly match internal table positions')
        if (nrow(pvb) != nrow(object@gatk) || any(pvb$refnt != object@gatk$refnt))
            stop('model file', path, 'does not exactly match internal table refnt')
        if (nrow(pvb) != nrow(object@gatk) || any(pvb$altnt != object@gatk$altnt))
            stop('model file', path, 'does not exactly match internal table altnt')
        pvb <- pvb[,.(abc.pv, lysis.pv, lysis.beta, mda.pv, mda.beta)]
    }

    object@gatk[, c('abc.pv', 'lysis.pv', 'lysis.beta', 'mda.pv', 'mda.beta') := pvb]
    object@mut.models <- data.frame(sites=nrow(pvb))
    object
})


setGeneric("compute.fdr.prior.data", function(object, mode=c('legacy', 'new'), quiet=FALSE)
    standardGeneric("compute.fdr.prior.data"))
setMethod("compute.fdr.prior.data", "SCAN2", function(object, mode=c('legacy', 'new'), quiet=FALSE)
{
    check.slots(object, c('gatk', 'static.filter.params'))
    # ALL candidates must be present for FDR estimation. So this function cannot be applied
    # to a chunked SCAN2 object (these are used for parallelization).
    check.chunked(object, 'compute.fdr.prior must be called on a SCAN2 object containing all sites, not a chunked parallelized object')

    mode <- match.arg(mode)
    if (!('static.filter' %in% colnames(object@gatk)))
        stop('must compute static filters before running compute.fdr.prior.data')

    muttypes <- c('snv', 'indel')
    object@fdr.prior.data <- c(setNames(lapply(muttypes, function(mt) {
        if (mode == 'legacy') {
            # in legacy mode, only candidate sites passing a small set of pre-genotyping
            # crtieria were used. also, notably, indel-specific hard filters (the only one
            # was dp >= 10 instead of dp >= 6) was not used here even though it should've
            # been.
            sfp <- object@static.filter.params[['snv']]
            cand <- object@gatk[
                muttype == mt &
                balt <= sfp$max.bulk.alt &
                bulk.gt == '0/0' &
                dbsnp == '.' &
                scalt >= sfp$min.sc.alt &
                dp >= sfp$min.sc.dp &
                bulk.dp >= sfp$min.bulk.dp &
                (is.na(balt.lowmq) | balt.lowmq <= sfp$max.bulk.alt)]
            hets=object@gatk[muttype == mt & training.site == TRUE & scalt >= sfp$min.sc.alt]
        } else {
            sfp <- object@static.filter.params[[mt]]
            # somatic.candidate already implies many non-single-cell-specific filters
            cand <- object@gatk[muttype == mt & somatic.candidate == TRUE & scalt >= sfp$min.sc.alt & dp >= sfp$min.sc.dp, .(af,dp)]
            # This is not supposed to be a highly filtered list. The filtering
            # should be somewhat equivalent to the pre-filtering steps on somatic
            # mutation candidate sites.
            #
            # Eventually, I think it would make most sense to apply the entire static
            # filter set to both candidates and hets (minus bulk alt filters, which
            # obviously will not work for hets) before computing FDR priors.
            hets <- object@gatk[muttype == mt & training.site == TRUE & scalt >= sfp$min.sc.alt & dp >= sfp$min.sc.dp & bulk.dp >= sfp$min.bulk.dp, .(af,dp)]
        }
    
        compute.fdr.prior.data.for.candidates(candidates=cand, hsnps=hets, random.seed=0, quiet=quiet, legacy=mode == 'legacy')
    }), muttypes), list(mode=mode))

    object
})


setGeneric("compute.fdr", function(object, path, mode=c('legacy', 'new'), quiet=FALSE)
    standardGeneric("compute.fdr"))
setMethod("compute.fdr", "SCAN2", function(object, path, mode=c('legacy', 'new'), quiet=FALSE)
{
    mode <- match.arg(mode)

    check.slots(object, c('gatk', 'ab.estimates', 'mut.models'))

    if (!missing(path) & !is.null(slot(object, 'fdr.prior.data')))
        stop('fdr.prior.data is already loaded; path to new fdr.prior.data would overwrite')

    if (!missing(path))
        object@fdr.prior.data <- get(load(path))

    check.slots(object, 'fdr.prior.data')

    muttypes <- c('snv', 'indel')
    object@fdr <- c(setNames(lapply(muttypes, function(mt) {
        # First use NT/NA tables to assign NT and NA to every site
        object@gatk[muttype == mt, c('nt', 'na') :=
            estimate.fdr.priors(dp=dp, af=af, prior.data=object@fdr.prior.data[[mt]], use.ghet.loo=FALSE)]

        # Re-assign NT/NA values to germline het training sites using a
        # leave-one-out appraoch.
        # Technically this should only be applied to the same sites used
        # as 'hets' in compute.fdr.prior.data.for.candidates(). 
        object@gatk[muttype == mt & training.site == TRUE &
            scalt >= object@static.filter.params[[mt]]$min.sc.alt, c('nt', 'na') :=
                estimate.fdr.priors(dp=dp, af=af, prior.data=object@fdr.prior.data[[mt]], use.ghet.loo=TRUE)]

        # lysis.fdr and mda.fdr represent the false discovery rate of a population of
        # candidate mutation sites with the same DP and VAF as the site in question.
        # Legacy computation finds min FDR over all alphas and is too slow for all sites.
        if (mode == 'legacy') {
            sfp <- object@static.filter.params[['snv']]
            # legacy computed FDR only for (almost) candidate sites and het germline
            # variants chosen for sensitivity calculation
            object@gatk[
                muttype == mt &
                (resampled.training.site |
                    (balt == 0 & bulk.gt == '0/0' &
                     dbsnp == '.' & scalt >= sfp$min.sc.alt &
                     dp >= sfp$min.sc.dp)),
                c('lysis.fdr', 'mda.fdr') := 
                    compute.fdr.legacy(altreads=scalt, dp=dp, gp.mu=match.ab(af=af, gp.mu=gp.mu),
                                       gp.sd=gp.sd, nt=nt, na=na, verbose=!quiet)]
                # legacy did NOT require passing min.bulk.dp or 0 bulk alt reads at low MQ
                #bulk.dp >= object@static.filter.params$min.bulk.dp]
                #(is.na(balt.lowmq) | balt.lowmq == 0)]
            sites <- nrow(object@gatk[
                muttype == mt &
                balt == 0 &
                bulk.gt == '0/0' &
                dbsnp == '.' &
                scalt >= sfp$min.sc.alt &
                dp >= sfp$min.sc.dp])
        } else if (mode == 'new') {
            # Useful hack: when subsetting a data.table (e.g., by muttype==mt above), the
            # simple arithmetic below causes a good deal of memory allocation.  However,
            # there is nothing mutation type-specific about the calculation below, so it
            # is equivalent to just assign to all sites regardless of muttype.
            object@gatk[, c('lysis.fdr', 'mda.fdr') :=
                list(lysis.pv*na / (lysis.pv*na + lysis.beta*nt),
                    mda.pv*na / (mda.pv*na + mda.beta*nt))]
            sites <- object@gatk[, sum(muttype == mt)]
        }
        list(sites=sites)
    }), muttypes), list(mode=mode))

    object
})

# To make MDA data usable, filter out insertions in homopolymers.
# In the future, it will be better to use a more flexible method to remove
# artifacts like this.
mda.indel.artifact.mutsigs <- c(
    # The most obvious artifact (from examining the ID83 spectrum) is 1bp
    # insertions (either type, C or T).
    paste0('1:Ins:C:', 3:5),
    paste0('1:Ins:T:', 3:5),
    # After removing the 1bp insertions, it becomes apparent that 2bp
    # insertions in homopolymers are also frequent in MDA calls, though at
    # MUCH lower levels.  My interpretation is that "2bp" insertions are
    # actually double hits of 1bp insertion artifacts.  When double hits 
    # occur in the same homopolymer run, there is no way to know if the
    # two insertions where adjacent (i.e., a 2bp insertion) or separate.
    #
    # My interpretation is supported by the composition of the "2bp insertions" 
    # when comparing indels from a set of >100 MDA neurons to ~80 PTA neurons.
    # The inserted bases are:
    #
    #   > sort(table(all.passing[amp=='MDA' & mutsig %in% paste0('2:Ins:R:', 2:5) & phenotype=='Control',substr(altnt,2,3)]))
    #   GC CT GA TC GG CC AG GT AC CA TG AT TA AA TT 
    #   2  5  8 11 12 14 15 19 21 30 31 49 63 91 96 
    #   > sort(table(all.passing[amp=='PTA' & mutsig %in% paste0('2:Ins:R:', 2:5) & phenotype=='Control',substr(altnt,2,3)]))
    #   AA AC AG CA GT TC TT GA TA 
    #   1  1  1  1  1  1  4  5  9 
    #
    # As expected, a majority (40%) of 2bp insertions are "AA" or "TT",
    # which are indistinguishable from 2 independent 1bp T insertions, the
    # most prevalent MDA artifact.
    paste0('2:Ins:R:', 2:5)
)

# legacy mode - id.score/hs.score both must be strictly > the id/hs.score.q cutoffs.
#               this was later found to be problematic for non-BWA MEM aligners that
#               generate no such CIGAR ops, causing all points to have the same scores.
#               in that case, allowing >= cutoff effectively disables the filter, which
#               is necessary.
setGeneric("compute.static.filters", function(object, mode=c('new', 'legacy'))
        standardGeneric("compute.static.filters"))
setMethod("compute.static.filters", "SCAN2", function(object, mode=c('new', 'legacy'))
{
    check.slots(object, c('gatk', 'cigar.data', 'mut.models'))
    mode <- match.arg(mode)

    object@static.filter.params$mode <- mode

    for (mt in c('snv', 'indel')) {
        sfp <- object@static.filter.params[[mt]]

        object@gatk[muttype == mt, c("cigar.id.test", "cigar.hs.test",
            "lowmq.test", "dp.test", "abc.test", "min.sc.alt.test", 
            "max.bulk.alt.test", "max.bulk.af.test", "max.bulk.binom.prob.test", 
            "dbsnp.test", "csf.test", 'mda.indel.artifact.test') :=
            list(id.score >= object@excess.cigar.scores[[mt]]$id.score.q,
                hs.score >= object@excess.cigar.scores[[mt]]$hs.score.q,
                is.na(balt.lowmq) | balt.lowmq <= sfp$max.bulk.alt,
                dp >= sfp$min.sc.dp & bulk.dp >= sfp$min.bulk.dp,
                abc.pv > 0.05,
                scalt >= sfp$min.sc.alt,
                balt <= sfp$max.bulk.alt,
                is.na(sfp$max.bulk.af) | bulk.af <= sfp$max.bulk.af,
                is.na(sfp$max.bulk.binom.prob) | bulk.binom.prob <= sfp$max.bulk.binom.prob,
                !sfp$exclude.dbsnp | dbsnp == '.',
                # is.na(unique.donors) - doesn't matter which field we pick from the
                # panel, if the site is missing all should be NA
                !sfp$panel.use | (is.na(unique.donors) & sfp$panel.allow.missing) | unique.donors <= sfp$panel.max.unique.donors | max.out <= sfp$panel.max.out,
                amp(object) != "MDA" | !(mutsig %in% mda.indel.artifact.mutsigs)
                )]

        if (mode == 'legacy') {
            object@gatk[muttype == mt, c('cigar.id.test', 'cigar.hs.test') := 
                list(id.score > idq[muttype], hs.score > hsq[muttype])]
        }

    }
    object@gatk[, static.filter :=
        cigar.id.test & cigar.hs.test & lowmq.test & dp.test &
        abc.test & min.sc.alt.test & max.bulk.alt.test & max.bulk.af.test & max.bulk.binom.prob.test &
        dbsnp.test & csf.test & mda.indel.artifact.test]
    object
})


# When reading the integrated table: read all metadata columns
# and only the 3 genotype/count columns corresponding to `sample.id`. This can
# save significant memory overhead in large (100+ cell) projects.  This optimized
# reading is handled by read.integrated.table.1sample().
#
# Next, annotate the integrated table with information corresponding to
# `sample.id`. Currently, only computes af, dp and assigns and assigns single cell
# ref/alt read counts to phased haplotypes.
#
# NO LONGER: includes training site definitions (which varies from cell
# to cell depending on whether there is no data at the site in that single cell;
# gt=./.). This is now done in the integrated table because that's where training
# site resampling occurs, so training sites must clearly be decided before that.
# Resampling should occur at the integrated table stage so that a consistent set
# of resampled training sites is used for all samples.
#
# N.B. parsimony phasing (adjust.phase()) used to be called here, but that is not
# a good idea. That should happen in the make.integrated.table pipeline where the
# full single cell+bulk count table is available. Information shared across single
# cells is useful to improve phasing and the final phase decision should be
# consistent across single cells.
read.and.annotate.integrated.table <- function(path, sample.id, region=NULL, quiet=FALSE) {
    tr <- read.integrated.table.1sample(path, sample.id, region=region, quiet=quiet)
    setindex(tr, resampled.training.site)

    # Add some convenient calculations
    tr[, dp := scalt + scref]
    tr[, af := scalt / dp]
    data.table::setkey(tr, chr, pos, refnt, altnt)
    setindex(tr, muttype) # allow for fast selection of SNVs or indels

    #sc.gt <- tr[[sample.id]]  # the column named after the sample is the GATK GT string for that sample
    #tr[, training.site := (!is.na(phased.gt) & (phased.gt == '1|0' | phased.gt == '0|1')) & sc.gt != './.' & bulk.gt != './.']
    tr[, c('phased.hap1', 'phased.hap2') :=
        list(ifelse(phased.gt == '0|1', scref, scalt),
             ifelse(phased.gt == '0|1', scalt, scref))]
    tr
}


read.training.hsnps <- function(path, sample.id, region=NULL, quiet=FALSE) {
    read.and.annotate.integrated.table(path=path, sample.id=sample.id, region=region, quiet=quiet)[training.site == TRUE & muttype == 'snv']
}


# After all site-specific information is computed, somatic mutations can
# be called.
#
# Somatic mutation calling involves determining FDR prior distributions
# (which are defined by the set of candidate sites), calculating FDR
# heuristics and finally passing sites based on a set of filters.
#
# The most important thing to note about calling mutations is
# that calls depend on the set of candidate sites defined by static
# filter parameters. That is, if you change something like the minimum alt
# reads per single cell, you will in turn change the set of possible
# candidate sites and need to rerun this function.
#
# N.B. because call.mutations must not be called on chunked SCAN2 objects,
# mode=legacy is not computationally feasible.
setGeneric("call.mutations", function(object, target.fdr=0.01, quiet=FALSE)
        standardGeneric("call.mutations"))
setMethod("call.mutations", "SCAN2", function(object, target.fdr=0.01, quiet=FALSE)
{
    check.slots(object, c('gatk', 'static.filter.params', 'mut.models',
                          'excess.cigar.scores', 'fdr.prior.data', 'fdr'))

    # Try to handle indel calling when the cross-sample filter does not meet
    # the usual requirements (>1 unique individual).
    #   1. If there is only one unique individual but several cells from that
    #      individual, then just discard indels shared across cells. With a
    #      few cells present this likely is a decent filter.
    #   2. If there is only one cell from one individual, indel calling should
    #      not be attempted.
    suppress.shared.indels <- max(object@gatk$unique.donors, na.rm=TRUE) == 1
    # <= 2: bulks are actually counted in unique.cells, so the case where
    # there is only one cell from one individual have cause max(unique.cells)=2
    suppress.all.indels <- max(object@gatk$unique.cells, na.rm=TRUE) <= 2

    # Pass somatic mutations
    object@gatk[, pass := static.filter == TRUE &
        (muttype == 'snv' | (!suppress.all.indels & (!suppress.shared.indels | unique.cells == 1))) &
        lysis.fdr <= target.fdr & mda.fdr <= target.fdr]

    # Pass germline heterozygous sites using L-O-O for sensitivity estimation
    object@gatk[training.site == TRUE,
        training.pass := 
            # same tests as static.filter EXCEPT:
            #  "lowmq.test"
            #  "max.bulk.alt.test", "max.bulk.af.test", "max.bulk.binom.prob.test", 
            #  "dbsnp.test", "csf.test", 'mda.indel.artifact.test'
            # which mostly will fail at training het germline sites.
            #
            # of particular interest is mda.indel.artifact.test: ~25% of all
            # germline population level indels are in the MDA artifact signature.  if
            # these sites were set to pass=FALSE, it could underestimate the total
            # indel burden if somatic indels are generally NOT in the artifact sig.
            # that, however, will change depending on cell type/condition, so there is
            # no way to know at this point in the pipeline.  the reason for ultimately
            # making this decision is SPATIAL sensitivity: germline indels are rarer than
            # germline SNPs, occurring closer to once per ~10kb rather than once per ~1kb.
            # Thus, we place greater value on the fact that a germline call violating the
            # artifact filter is evidence that SCAN2 is powered to detect indels GENERALLY
            # in the ~10kb region.
            #
            # A better solution for sensitivity calculation would be:
            #   1. fail germline indels that violate the mda filter (set pass=FALSE)
            #   2. followed by an adjustment to sensitivity estimation: downsample
            #      germline indels to match the somatic pass spectrum and calculate
            #      sens.  The problem with this approach is downsampling will fail for
            #      cells with very few somatic indels (some even have 0).  A possible
            #      solution is to do something akin to bootstrapping: downsample the
            #      germline indels to make somatic pass indels 100s or 1000s of times,
            #      calculating a sensitivity each time.  Hopefully that would yield a
            #      stable sens. estimate.
            (cigar.id.test & cigar.hs.test & dp.test & abc.test & min.sc.alt.test) &
            lysis.fdr <= target.fdr & mda.fdr <= target.fdr]

    # Rescue hasn't happened (or if it has, reset it), so introduce the rescue columns
    # to prevent a data.table issue where columns cannot be added to data.tables
    # load()ed from file without a full copy.
    object@gatk[, c('rescue.candidate', 'rweight', 'rescue.fdr', 'rescue') :=
        list(as.logical(FALSE), as.numeric(NA), as.numeric(NA), as.logical(FALSE))]


    # NAs are especially present in legacy output where not all sites
    # have CIGAR data, not all sites have FDR values and sites not in
    # the cross sample panel (which were originally removed by merge())
    # may have NAs (though they shouldn't; 0-fills should be used).
    snv.pass <- object@gatk[, sum(muttype == 'snv' & pass == TRUE, na.rm=TRUE)]
    snv.resampled.training.pass <- object@gatk[, sum(muttype == 'snv' & resampled.training.site == TRUE & training.pass == TRUE, na.rm=TRUE)]
    indel.pass <- object@gatk[, sum(muttype == 'indel' & pass == TRUE, na.rm=TRUE)]
    indel.resampled.training.pass <- object@gatk[, sum(muttype == 'indel' & resampled.training.site == TRUE & training.pass == TRUE, na.rm=TRUE)]
    object@call.mutations <- list(
        snv.pass=snv.pass,
        snv.resampled.training.pass=snv.resampled.training.pass,
        indel.pass=indel.pass,
        indel.resampled.training.pass=indel.resampled.training.pass,
        target.fdr=target.fdr,
        suppress.shared.indels=suppress.shared.indels,
        suppress.all.indels=suppress.all.indels)
    object
})


setGeneric("add.depth.profile", function(object, depth.path)
        standardGeneric("add.depth.profile"))
setMethod("add.depth.profile", "SCAN2", function(object, depth.path) {
    vars <- load(depth.path)  # should contain two objects: dptab and clamp.dp
    if (!all(c('dptab', 'clamp.dp') %in% vars))
        stop('depth.profile RDA file expected to contain dptab and clamp.dp objects')

    object@depth.profile <- list(
        dptab=dptab,
        dptabs.sex=dptabs.sex,
        clamp.dp=clamp.dp
    )
    object
})


setGeneric("add.binned.counts", function(object, sc.path, bulk.path, gc.path)
        standardGeneric("add.binned.counts"))
setMethod("add.binned.counts", "SCAN2", function(object, sc.path, bulk.path, gc.path) {
    object@binned.counts <- list(
        sc=read.binned.counts(bin.path=sc.path, gc.path=gc.path),
        bulk=read.binned.counts(bin.path=bulk.path, gc.path=gc.path)
    )
    object
})


# neighborhood.tiles - calculate a rolling sum of the number of training sites using
#   a window extending `neighborhood.tiles` upstream and downstream of a given tile.
#   The window will be of size 1 (the center window) + 2*neighborhood.tiles.
#   This quantifies how much data is available to the AB model in addition to the
#   standard deviation of the Gaussian process model (gp.sd).
#   For PTA data, covariance analysis suggests most local AB information is within
#   10 kB. Thus, with the default sens.tilewidth=1kb, neighborhood.tiles=10 should
#   be a reasonable default.
setGeneric("add.sensitivity.covariates", function(object, abmodel.covs.path, depth.covs.path, neighborhood.tiles=10)
        standardGeneric("add.sensitivity.covariates"))
setMethod("add.sensitivity.covariates", "SCAN2", function(object, abmodel.covs.path, depth.covs.path, neighborhood.tiles=10)
{
    cov.data <- cbind(read.tabix.data(abmodel.covs.path),
        read.tabix.data(depth.covs.path)[,-(1:3)])

    # Add an extra indicator variable for each sex chromosome.  This allows
    # sex chroms to be analyzed along with autosomes despite likely differences
    # in sensitivity.  There is one indicator per sex chrom to reflect:
    #
    #   for females: chrX is diploid and any data on chrY is likely noise
    #   for males: although chrX and chrY are both haploid, chrY seems to
    #       have very serious alignment problems and relatively few het SNPs
    #       to act as training sites.
    for (sex.chrom in get.sex.chroms(object)) {
        cov.data[[paste0('is.', sex.chrom)]] <- cov.data$chr == sex.chrom
    }

    sens.regions <- GenomicRanges::GRanges(seqnames=cov.data$chr,
        ranges=IRanges::IRanges(start=cov.data$start, cov.data$end),
        seqinfo=genome.string.to.seqinfo.object(object@genome.string))

    for (mt in c('snv', 'indel')) {
        cat('Counting germline training', mt, 'sites..\n')
        hets <- count.germline.sites.for.sens(grs=sens.regions,
            sites=object@gatk[training.site == TRUE & muttype == mt],
            seqinfo=genome.string.to.seqinfo.object(object@genome.string),
            neighborhood.tiles=neighborhood.tiles)
        colnames(hets) <- paste0(mt, '.', colnames(hets))
        cov.data <- cbind(cov.data, hets)

        cat('Counting somatic', mt, 'calls..\n')
        calls <- count.somatic.sites.for.sens(gr=sens.regions,
            sites=object@gatk[pass == TRUE & muttype == mt],
            seqinfo=genome.string.to.seqinfo.object(object@genome.string),
            neighborhood.tiles=neighborhood.tiles)
        colnames(calls) <- paste0(mt, '.', colnames(calls))
        cov.data <- cbind(cov.data, calls)
    }

    object@spatial.sensitivity <- list(
        data=cov.data,
        tilewidth=cov.data[1,end-start+1],
        neighborhood.tiles=neighborhood.tiles
    )
    object
})


setGeneric("compute.sensitivity.models", function(object, seed=0) standardGeneric("compute.sensitivity.models"))
setMethod("compute.sensitivity.models", "SCAN2", function(object, seed=0) {
    data <- object@spatial.sensitivity$data

    # Split data into two halves for later hold-out training
    set.seed(seed)
    data[, hold.out := rbinom(nrow(.SD), size=1, prob=1/2)]

    # Standardized Z-score for depth - there is still an issue where depth is generally
    # positively correlated with sensitivity until extremely high depth is reached.
    data[, norm.mean.sc.dp := ifelse(mean.sc.dp==0, NA, log10(mean.sc.dp))]
    data[, norm.mean.sc.dp := (norm.mean.sc.dp - mean(norm.mean.sc.dp, na.rm=TRUE))/sd(norm.mean.sc.dp, na.rm=TRUE)]

    # Each call to model.somatic.sensitivity updates `ret`.
    # IMPORTANT: the returned model objects are HUGE (~1 GB each) because copies
    # of the entire input dataset are retained along with several O(#rows)-sized
    # calculations.  Recomputing these models only takes a few seconds given `ret`,
    # so only record the final coefficients (and the predictions, which are added
    # to `data` by reference).
    cat("Computing models..\n")
    models <- setNames(do.call(c,
        lapply(c('snv', 'indel'), function(muttype)
            lapply(c('maj', 'min'), function(alleletype)
                coef(summary(model.somatic.sensitivity(tiles=data, muttype=muttype, alleletype=alleletype, sex.chroms=get.sex.chroms(object))))
        ))
    ), c('snv.maj', 'snv.min', 'indel.maj', 'indel.min'))

    cat("Inferring burden..\n")
    burdens <- setNames(lapply(c('snv', 'indel'), function(mt)
        estimate.burden.by.spatial.sensitivity(data=data, muttype=mt)),
        c('snv', 'indel'))

    cat("Summarizing..\n")
    ss <- somatic.sensitivity(data=data)

    object@spatial.sensitivity$models <- models
    object@spatial.sensitivity$somatic.sensitivity <- ss
    object@spatial.sensitivity$burden <- burdens

    object
})
